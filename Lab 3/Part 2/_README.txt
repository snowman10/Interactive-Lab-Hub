# Chatterboxes

**NAMES OF COLLABORATORS HERE**

Kazim Jafri (khj23)\
Rei Chen (rc884)\
Zixin Li (zl865)\
Rowan Wu (rww99)\
Arystan Tatishev (at855)

## Prep for Part 1: Get the Latest Content and Pick up Additional Parts 

### Pick up Web Camera If You Don't Have One

**Please note:** connect the webcam/speaker/microphone while the pi is *off*. 

### Get the Latest Content

## Part 1.
### Setup 

### Text to Speech 

\*\***Write your own shell file to use your favorite of these TTS engines to have your Pi greet you by name.**\*\*
  
### Speech to Text

\*\***Write your own shell file that verbally asks for a numerical based input (such as a phone number, zipcode, number of pets, etc) and records the answer the respondent provides.**\*\*

### Serving Pages

### Storyboard

\*\***Post your storyboard and diagram here.**\*\*

\*\***Please describe and document your process.**\*\*

### Acting out the dialogue

\*\***Describe if the dialogue seemed different than what you imagined when it was acted out, and how.**\*\*

### Wizarding with the Pi (optional)

\*\***Describe if the dialogue seemed different than what you imagined, or when acted out, when it was wizarded, and how.**\*\*

# Lab 3 Part 2

For Part 2, you will redesign the interaction with the speech-enabled device using the data collected, as well as feedback from part 1.

## Prep for Part 2

1. What are concrete things that could use improvement in the design of your device? For example: wording, timing, anticipation of misunderstandings...
2. What are other modes of interaction _beyond speech_ that you might also use to clarify how to interact?
3. Make a new storyboard, diagram and/or script based on these reflections.

## Prototype your system

The system should:
* use the Raspberry Pi 
* use one or more sensors
* require participants to speak to it. 

*Document how the system works*

*Include videos or screencaptures of both the system and the controller.*

## Test the system
Try to get at least two people to interact with your system. (Ideally, you would inform them that there is a wizard _after_ the interaction, but we recognize that can be hard.)

Answer the following:

### What worked well about the system and what didn't?
\*\**your answer here*\*\*

What worked well: The device automatically responds to the user in the language it is spoken to, so it does not need the user to manually select a language he/she would like to speak in. 

What didn’t: It might be hard for users to know and speak to the device in their native language as people often would just speak in English in foreign countries if they don’t know the language there. We wanted to display the user's input onto the screen, however we found that Chinese characters could not be displayed. Other languages worked though. 

### What worked well about the controller and what didn't?
\*\**your answer here*\*\*

What went well: The keypad could be used by the user when he/she approaches our device to start/end recording what they want to say, so the device can respond directly to what the user is asking it, instead of responding to the user’s conversation to other people. 
 
What didn’t: Sometimes there would be lags in the speech from the speaker and text on the screen. 

### What lessons can you take away from the WoZ interactions for designing a more autonomous version of the system?
\*\**your answer here*\*\*

### How could you use your system to create a dataset of interaction? What other sensing modalities would make sense to capture?
\*\**your answer here*\*\*